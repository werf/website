In this section, we will show you how to manage the application lifecycle.

## Stopping processes in containers

The signal specified in `STOPSIGNAL` (usually, the `TERM` signal) is sent to the process to stop it. However, some applications cannot handle it properly and cannot manage to shut down gracefully. The same is true for applications running in Kubernetes.

For example, in order to shut down nginx properly, you will need a `preStop` hook like this:

```yaml
lifecycle:
  preStop:
    exec:
      command:
      - /bin/sh
      - -ec
      - |
        sleep 3
        nginx -s quit
```

A brief explanation for this listing:

1. `sleep 3` prevents race conditions that may be caused by an endpoint being deleted.
1. `nginx -s quit` shuts down nginx properly. This line isn’t required for more up-to-date images since the `STOPSIGNAL: SIGQUIT` parameter is set there by default.

>You can learn more about graceful shutdowns for nginx bundled with PHP-FPM in [our other article](https://blog.flant.com/graceful-shutdown-in-kubernetes-is-not-always-trivial/).

The way `STOPSIGNAL` is handled depends on the application itself. In practice, for most applications, you have to Google the way `STOPSIGNAL` is handled. If the signal is not handled appropriately the `preStop` hook can help you solve the problem. Another option is to replace `STOPSIGNAL` with a signal that the application can handle properly (and permit it to shut down gracefully).

`terminationGracePeriodSeconds` is another crucial parameter important in shutting down the application. It specifies the time period for which the application is to shut down gracefully. If the application does not terminate within this time frame (30 seconds by default), it will receive a `KILL` signal. Thus, you will need to increase the terminationGracePeriodSeconds parameter if you think that running the `preStop` hook and/or shutting down the application at the `STOPSIGNAL` may take more than 30 seconds. For example, you may need to increase it if some requests from your web service clients take a long time to complete (e.g. requests that involve downloading large files).

It is worth noting that the `preStop` hook has a locking mechanism, i.e. `STOPSIGNAL` may be sent only after the `preStop` hook has finished running. At the same time, the `terminationGracePeriodSeconds` countdown *continues during the preStop hook execution*. All the hook-induced processes, as well as the processes running in the container, will be `KILL`ed after `terminationGracePeriodSeconds` is over.

Also, some applications have specific settings that set the deadline at which point the application must terminate (for example, the `--timeout` option in Sidekiq). Therefore, in each case, you have to make sure that if the application has this setting, it has a slightly lower value than that of `terminationGracePeriodSeconds`.

## Liveness probe

In practice, the liveness probe is not as widely used as you may have thought. Its purpose is to restart a container if, for example, the application is frozen. However, in real life, such app deadlocks are an exception rather than the rule. If the application demonstrates partial functionality for some reason (e.g., it cannot restore connection to a database after it has been broken), you have to fix that in the application, rather than “inventing” livenessProbe-based workarounds.

> **Общая рекомендация для всех проб:** выставляйте высокий `timeoutSeconds`. Значение по умолчанию в одну секунду — слишком низкое. Особенно критично для `readinessProbe` и `livenessProbe`. Слишком низкий `timeoutSeconds` будет приводить к тому, что при увеличении времени ответов у приложений в Pod'ах (что обычно происходит для всех Pod'ов сразу благодаря балансированию нагрузки с помощью Service) либо перестанет приходить трафик почти во все Pod'ы (readiness), либо, что ещё хуже, начнутся каскадные перезапуски контейнеров (liveness).

While you can use livenessProbe to check for these kinds of states, we recommend either **not using livenessProbe by default** or only performing some **basic liveness-testing, such as testing for the TCP connection (remember to set a high timeout value)**. This way, the application will be restarted in response to an apparent deadlock without risking falling into the trap of a loop of restarts (i.e. restarting it won’t help).

Risks related to a poorly configured livenessProbe are serious. In the most common cases, livenessProbe fails due to increased load on the application (it simply cannot make it within the time specified by the timeout parameter) or due to the state of external dependencies that are currently down being checked (directly or indirectly). In the latter case, all the containers will be restarted. In the best case scenario, this would result in nothing, but in the worst case, this would render the application  completely unavailable, probably long-term. Long-term total unavailability of an application (if it has a large number of replicas) may result if most Pods’ containers are restarted within a short time period. Some containers are likely to become READY faster than others, and the entire load will be distributed over this limited number of running containers. That will end up causing livenessProbe timeouts, which will trigger even more restarts.

Also, ensure that livenessProbe does not stop responding if your application has a limit on the number of established connections and that limit has been reached. Usually, you have to dedicate a separate application thread/process to livenessProbe to avoid such problems. For example, if your application has 11 threads (one thread per client), you can limit the number of clients to 10, ensuring that there is an idle thread available for livenessProbe.

And, of course, do not add any external dependency checks to your livenessProbe.

>See [this article](https://srcco.de/posts/kubernetes-liveness-probes-are-dangerous.html) for more information on liveness probe issues and how to prevent them.